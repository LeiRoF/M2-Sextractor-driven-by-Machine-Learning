{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, Dense, Dropout, Flatten \n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# import pandas as pd \n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import pickle\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# import cv2\n",
    "import json\n",
    "import Step_0_Config as config\n",
    "\n",
    "def get_conv_model(dim = (150,150, 3)):\n",
    "    '''This function will create and compile a CNN given the input dimension'''\n",
    "    inp_shape = dim\n",
    "    act = 'relu'\n",
    "    drop = .25\n",
    "    kernal_reg = regularizers.l1(.001)\n",
    "    optimizer = Adam(lr = .0001)    \n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape, \n",
    "                     kernel_regularizer = kernal_reg,\n",
    "                     kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),  strides = (3,3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (3,3))) \n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (3,3)))  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(3, activation='softmax', name = 'Output_Layer'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "\n",
    "for file in os.listdir(path := \"data/dataset\"):\n",
    "\n",
    "    # Considering only files that are ready to be exploited\n",
    "    if \"ai_ready.json\" in file:\n",
    "\n",
    "        # Loading labels\n",
    "        with open(path + \"/\" + file) as f:\n",
    "            labels.append(json.load(f))\n",
    "\n",
    "        # Loading pictures\n",
    "        data.append(\n",
    "            np.load(\n",
    "                path + \"/\" + file.replace(\"_ai_ready.json\", \".npz\")\n",
    "            )[\"picture\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(data[:int(len(data)*.8)])\n",
    "train_labels = np.array(labels[:int(len(labels)*.8)])\n",
    "test_data = np.array(data[int(len(data)*.8):])\n",
    "test_labels = np.array(labels[int(len(labels)*.8):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Dev\\CompuPhys\\Sextractor-driven-by-Machine-Learning\\_sextractor\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type dict).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m get_conv_model(dim\u001b[39m=\u001b[39m(config\u001b[39m.\u001b[39mN, config\u001b[39m.\u001b[39mN, \u001b[39m1\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_data, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(test_data, test_labels))\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Dev\\CompuPhys\\Sextractor-driven-by-Machine-Learning\\_sextractor\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Dev\\CompuPhys\\Sextractor-driven-by-Machine-Learning\\_sextractor\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type dict)."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "model = get_conv_model(dim=(config.N, config.N, 1))\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('_sextractor': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.8"
=======
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
>>>>>>> c2b87e0c12f0864061ca4daa02ace67056400dee
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ddbb7d6da99652709240c4018ccfcb2db29485173f17c664a2460dd7ec13656"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
